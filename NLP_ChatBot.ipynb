{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyMpe4e104LkhGqzRyONjZu0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranckCode/nlp-chatbot/blob/main/NLP_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "9qoH3Ca4LK9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "Ri95KjWsDqP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "iA-r8wVFJW4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenizer = Tokenizer(filters='', lower=True)\n",
        "\n",
        "# Chemin vers le fichier JSON\n",
        "dataset_file = 'dataset.json'"
      ],
      "metadata": {
        "id": "e-og9tduJm_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prétraitement des données\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Convertir en minuscules\n",
        "    text = text.lower()\n",
        "\n",
        "    # Supprimer les caractères de ponctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Suppression des chiffres\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Rejoindre les tokens en une seule chaîne de texte\n",
        "    preprocessed_text = \" \".join(tokens)\n",
        "\n",
        "    return preprocessed_text\n"
      ],
      "metadata": {
        "id": "nVfiSvEqJqY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_and_fit_modele():\n",
        "\n",
        "    data=[]\n",
        "\n",
        "    # Ouvrir le fichier JSON\n",
        "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(\" ---> Début du Chargment des données\")\n",
        "\n",
        "    # Chargement des questions-réponses depuis la base de données\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    # Charger le champ \"question_response\"\n",
        "    questions_reponses = data['questions_responses']\n",
        "    for item in questions_reponses:\n",
        "        questions.append(item['question'])\n",
        "        answers.append(item['response'])\n",
        "\n",
        "    print(\" ---x Fin du Chargment des données \")\n",
        "\n",
        "    print(\" ---> Début du Pré-traitement des données\")\n",
        "\n",
        "    questions = [preprocess_text(question) for question in questions]\n",
        "    answers = [preprocess_text(answer) for answer in answers]\n",
        "\n",
        "    print(\" ---x Fin du Pré-traitement des données\")\n",
        "\n",
        "    print(\" ---> Début de l'entrainement des données\")\n",
        "\n",
        "    tokenizer.fit_on_texts(questions + answers + ['[START]', '[END]'])\n",
        "\n",
        "    print(\" ---x Fin de l'entrainement des données\")\n",
        "\n",
        "    print(\" ---> Début de la création des séquences\")\n",
        "\n",
        "    # Création des séquences d'index\n",
        "\n",
        "    # Initialise une liste vide pour stocker les séquences d'entrée\n",
        "    questions_sequences = []\n",
        "    answers_sequences = []\n",
        "\n",
        "    # Parcourt les réponses\n",
        "    for question in questions:\n",
        "        question_token_list = tokenizer.texts_to_sequences(question)[0]\n",
        "        for answer in answers:\n",
        "            # Convertit la réponse en une séquence d'index en utilisant le tokenizer\n",
        "            answer_token_list = tokenizer.texts_to_sequences(['[START] ' + answer + ' [END]'])[0]\n",
        "            # Parcourt la séquence d'index\n",
        "            for i in range(1, len(answer_token_list)):\n",
        "                # Crée une sous-séquence d'index à partir de la séquence d'index actuelle\n",
        "                n_gram_sequence = answer_token_list[:i+1]\n",
        "                answers_sequences.append(n_gram_sequence)\n",
        "                questions_sequences.append(question_token_list)\n",
        "\n",
        "    print(\" ---x Fin de la création des séquences\")\n",
        "\n",
        "    print(\" ---> Début du rembourrage des séquences\")\n",
        "\n",
        "    # Padding des séquences\n",
        "    max_sequence_length = max(len(seq) for seq in questions_sequences + answers_sequences)\n",
        "    questions_sequences = np.array(pad_sequences(questions_sequences, maxlen=max_sequence_length, padding='post'))\n",
        "    answers_sequences = np.array(pad_sequences(answers_sequences, maxlen=max_sequence_length, padding='post'))\n",
        "\n",
        "    print(\" ---x Fin du rembourrage des séquences\")\n",
        "\n",
        "    print(\" ---> Début de la création et l'entrainemment du modèle\")\n",
        "\n",
        "    # Modèle de chatbot\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    embedding_dim = 100\n",
        "    hidden_units = 256\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_sequence_length,))\n",
        "    encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_lstm = tf.keras.layers.Bidirectional(LSTM(hidden_units, return_sequences=True, return_state=True))\n",
        "    _, forward_state_h, forward_state_c, backward_state_h, backward_state_c = encoder_lstm(encoder_embedding)\n",
        "    state_h = tf.keras.layers.Concatenate()([forward_state_h, backward_state_h])\n",
        "    state_c = tf.keras.layers.Concatenate()([forward_state_c, backward_state_c])\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_sequence_length-1,))\n",
        "    decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(hidden_units*2, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Modèle complet\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    # Compilation du modèle\n",
        "    model.compile(optimizer=RMSprop(), loss='sparse_categorical_crossentropy')\n",
        "\n",
        "    # Entraînement du modèle\n",
        "    model.fit([questions_sequences, answers_sequences[:, :-1]], answers_sequences[:, 1:], epochs=1, batch_size=32)\n",
        "    print(\" ---x Fin de la création et l'entrainemment du modèle\")\n",
        "\n",
        "\n",
        "    print(\" ---> Début de la sauvegarde du modèle\")\n",
        "    # Sauvegarde le max_sequence_length\n",
        "    with open('max_sequence_length.json', 'w') as file:\n",
        "        json.dump(max_sequence_length, file)\n",
        "\n",
        "    # Sauvegarde du tokenizer\n",
        "    with open('tokenizer.pickle', 'wb') as file:\n",
        "        pickle.dump(tokenizer, file)\n",
        "\n",
        "    # Sauvegarde le modèle entier et le max_sequence_length\n",
        "    model.save('modele')\n",
        "    print(\" ---x Fin de la sauvegarde du modèle\")\n",
        ""
      ],
      "metadata": {
        "id": "gIzc0hxxJy8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_data_and_fit_modele()"
      ],
      "metadata": {
        "id": "S9VY3zCuK2RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test du modèle**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "esPl0AO7Vnsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"\"\n",
        "max_sequence_length = \"\"\n",
        "tokenizer = \"\""
      ],
      "metadata": {
        "id": "2OnGte_kVx2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "\n",
        "    global model, max_sequence_length, tokenizer\n",
        "\n",
        "    # Pour charger le modèle entier\n",
        "    model = tf.keras.models.load_model('modele')\n",
        "\n",
        "    # Chargement du max_sequence_length\n",
        "    with open('max_sequence_length.json', 'r') as file:\n",
        "        max_sequence_length = json.load(file)\n",
        "\n",
        "    # Chargement du tokenizer\n",
        "    with open('tokenizer.pickle', 'rb') as file:\n",
        "        tokenizer = pickle.load(file)"
      ],
      "metadata": {
        "id": "sYrWbhQXWIXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour générer une réponse à partir d'une question de l'utilisateur\n",
        "def generate_response(question):\n",
        "    # Prétraitement de la question de l'utilisateur\n",
        "    preprocessed_question = preprocess_text(question)\n",
        "\n",
        "    # Conversion en séquence d'index\n",
        "    question_sequence = tokenizer.texts_to_sequences([preprocessed_question])\n",
        "    question_sequence = pad_sequences(question_sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    # Prédiction de la réponse\n",
        "    answer_input = np.zeros((1, max_sequence_length-1))\n",
        "    answer_input[0, 0] = tokenizer.word_index['[start]']\n",
        "\n",
        "    for i in range(1, max_sequence_length-1):\n",
        "        prediction = model.predict([question_sequence, answer_input])\n",
        "        predicted_word_index = np.argmax(prediction[0, i-1, :])\n",
        "        answer_input[0, i] = predicted_word_index\n",
        "        # print(answer_input)\n",
        "\n",
        "        if predicted_word_index == tokenizer.word_index['[end]']:\n",
        "            break\n",
        "    # Conversion de la séquence d'index en réponse textuelle\n",
        "    answer = tokenizer.sequences_to_texts([answer_input[0, :i+1]])[0]\n",
        "    answer = answer.replace('[start]', '').replace('[end]', '').strip()\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "ZBoXXuewWPde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_model()\n",
        "while True:\n",
        "    user_question = input(\"\\n Quel est votre question ? \\t\")\n",
        "    response = generate_response(user_question)\n",
        "    print(\"User:\", user_question)\n",
        "    print(\"Bot:\", response)"
      ],
      "metadata": {
        "id": "VpLnqBDLWRbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}